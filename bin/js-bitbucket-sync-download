#!/usr/bin/env python3

import os
import subprocess

import logging
from typing import Dict, Set

from dotenv import load_dotenv

import requests
from requests.auth import HTTPBasicAuth
from time import sleep

logger = logging.getLogger(__name__)


def nice_sleep():
    sleep(int(os.environ.get("NICE_SLEEP")) / 1000)


def all_repos() -> Set[str]:
    logger.info("All repos: Running")
    username = os.environ.get("BITBUCKET_USER")
    password = os.environ.get("BITBUCKET_PASS")
    team = os.environ.get("BITBUCKET_TEAM")

    full_repo_list = set()

    # Request 100 repositories per page (and only their slugs), and the next page URL
    next_page_url = f"https://api.bitbucket.org/2.0/repositories/{team}?pagelen=100&fields=next,values.links.clone.href,values.slug"

    # Keep fetching pages while there's a page to fetch
    while next_page_url is not None:
        response = requests.get(next_page_url, auth=HTTPBasicAuth(username, password))
        page_json = response.json()

        # Parse repositories from the JSON
        for repo in page_json['values']:
            cloner: Dict[str]
            for cloner in repo['links']['clone']:
                if not cloner["href"].startswith("git@"):
                    continue
                git_clone = cloner["href"]
                logger.info(f"All repos: Found repo {git_clone}")
                full_repo_list.add(git_clone)

        # Get the next page URL, if present
        # It will include same query parameters, so no need to append them again
        next_page_url = page_json.get('next', None)
        logger.info(f"All repos: Next page")
        nice_sleep()

    logger.info(f"All repos: Found {len(full_repo_list)} repo(s)")

    return full_repo_list


def clone_repo(clone_url: str):
    logging.info(f"Cloning '{clone_url}'")

    my_env = os.environ.copy()
    my_env["SKIP_ID_OPEN"] = "yes"

    command = ["js-git-clone", clone_url]
    # subprocess.call(command, env=my_env)

    output = ""
    p = subprocess.Popen(command, stdout=subprocess.PIPE, env=my_env)
    for line in iter(p.stdout.readline, b''):
        logger.info(str(line).strip("\n"))
        output += str(line)
    p.stdout.close()
    p.wait()

    # Sleep only if we have not cloned the repo before
    if output.count("Directory already exists, skipping clone") <= 0:
        nice_sleep()


def main():
    dir_home = os.environ.get('HOME')
    project_dir = os.path.realpath(os.path.dirname(os.path.realpath(__file__)) + "/..")
    load_dotenv(dotenv_path=f"{project_dir}/.env")

    logging.basicConfig(level=os.environ.get("LOGLEVEL", "INFO"))
    logger.info(f"Home dir of '{dir_home}' and project dir or '{project_dir}'")

    repos = all_repos()
    repos_count = len(repos)
    for i, repo in enumerate(repos):
        logger.info(f"Repo number {str(i + 1)}/{str(repos_count)}")
        clone_repo(repo)
        logger.info("")

    logging.info(f"Done!")


if __name__ == '__main__':
    main()
